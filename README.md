# Awesome-Transformers
A list of transformers

1. **Attention Is All You Need**, NIPS 2017 [(paper)](https://arxiv.org/pdf/1706.03762.pdf) 
_originial paper_

1. **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**, ACL 2019 [(paper)](https://arxiv.org/pdf/1901.02860.pdf) [(pytorch & tensorflow code)](https://github.com/kimiyoung/transformer-xl)
  _segment-level recurrence, and relative position encoding_

1. **COMET: Commonsense Transformers for Automatic Knowledge Graph Construction**, ACL 2019 [(paper)](https://arxiv.org/pdf/1906.05317.pdf) [(pytorch code)](https://github.com/atcbosselut/comet-commonsense)
  _used for KG's tuples_

1. **Adaptive Attention Span in Transformers**, ACL 2019 [(paper)](https://arxiv.org/pdf/1905.07799.pdf) [(pytorch code)](https://github.com/facebookresearch/adaptive-span)
  _adaptive attention span_

1. **XLNet: Generalized Autoregressive Pretraining for Language Understanding**, arxiv 2019 [(paper)](https://arxiv.org/pdf/1906.08237.pdf) [(tensorflow code)](https://github.com/zihangdai/xlnet)
  _permutation language model_

1. **Syntactically Supervised Transformers for Faster Neural Machine Translation**, ACL 2019 [(paper)](https://www.aclweb.org/anthology/P19-1122) [(pytorch code)](https://github.com/dojoteef/synst)
  _non-autoregressive decoding_

1. **Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction**, ACL 2019 [(paper)](https://www.aclweb.org/anthology/P19-1134) [(pytorch code)](https://github.com/DFKI-NLP/DISTRE)
  _relation extraction_

1. **Learning Deep Transformer Models for Machine Translation**, ACL 2019 [(paper)](https://www.aclweb.org/anthology/P19-1176) [(pytorch code)](https://github.com/wangqiangneu/dlcl)
  _residual_

1. **Large Batch Optimization for Deep Learning: Training BERT in 76 Minutes**, arxiv 2019 [(paper)](https://arxiv.org/pdf/1904.00962.pdf) _distributed computation_ 
