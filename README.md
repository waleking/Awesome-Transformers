# Awesome-Transformers
A list of transformers

1. **Attention Is All You Need**, NIPS 2017 [(paper)](https://arxiv.org/pdf/1706.03762.pdf) 
  originial paper

1. **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**, ACL 2019 [(paper)](https://arxiv.org/pdf/1901.02860.pdf) [(pytorch & tensorflow code)](https://github.com/kimiyoung/transformer-xl)
  segment-level recurrence, and relative position encoding

1. **COMET: Commonsense Transformers for Automatic Knowledge Graph Construction**, ACL 2019 [(paper)](https://arxiv.org/pdf/1906.05317.pdf) [(pytorch code)](https://github.com/atcbosselut/comet-commonsense)
  used for KG's tuples

1. **Adaptive Attention Span in Transformers**, ACL 2019 [(paper)](https://arxiv.org/pdf/1905.07799.pdf) [(pytorch code)](https://github.com/facebookresearch/adaptive-span)
  adaptive attention span

1. **XLNet: Generalized Autoregressive Pretraining for Language Understanding**, arxiv 2019 [(paper)](https://arxiv.org/pdf/1906.08237.pdf) [(tensorflow code)](https://github.com/zihangdai/xlnet)
  permutation language model

1. **Syntactically Supervised Transformers for Faster Neural Machine Translation**, ACL 2019 [(paper)](https://www.aclweb.org/anthology/P19-1122) [(pytorch code)](https://github.com/dojoteef/synst)
  non-autoregressive decoding

1. **Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction**, ACL 2019 [(paper)](https://www.aclweb.org/anthology/P19-1134) [(pytorch code)](https://github.com/DFKI-NLP/DISTRE)
  relation extraction

1. **Learning Deep Transformer Models for Machine Translation**, ACL 2019 [(paper)](https://www.aclweb.org/anthology/P19-1176) [(pytorch code)](https://github.com/wangqiangneu/dlcl)
  residual
